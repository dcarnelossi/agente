import json

from database import PostgresDB
from langchain.schema import AIMessage, HumanMessage
from langchain_community.agent_toolkits import SQLDatabaseToolkit
from langchain_community.tools.sql_database.tool import QuerySQLCheckerTool
from langgraph.types import Command, interrupt
from llm import model
from prompt import (
    format_answer_prompt,
    get_classification_prompt,
    get_visualization_prompt,
    interact_prompt,
    sql_prompt,
)


class EcommerceAgent:
    def __init__(self):
        self.db = PostgresDB()
        self.toolkit = SQLDatabaseToolkit(db=self.db.db, llm=model)
        self.query_checker = QuerySQLCheckerTool(db=self.db.db, llm=model)  # ðŸ”¹ Instancia o validador SQL

    def collect_user_interaction(self, state: dict) -> Command:
        """FunÃ§Ã£o para interromper e coletar a entrada do usuÃ¡rio."""
        entrada_usuario = interrupt(value="Aguardando a entrada do usuÃ¡rio.")
        neo = state["messages"].append(entrada_usuario)
        return Command(update=neo, goto="interact_with_user")

    def interact_with_user(self, state: dict) -> dict:
        """
        Interage com o usuÃ¡rio e auxilia na formulaÃ§Ã£o de uma pergunta vÃ¡lida.
        Utiliza um prompt de sistema para guiar a conversa e garantir respostas consistentes.
        """

        user_message = state["messages"][-1]  # Ãšltima mensagem do usuÃ¡rio

        # ðŸ”¹ ObtÃ©m o System Prompt
        system_prompt = interact_prompt()

        # ðŸ”¹ Criamos a lista de mensagens que o LLM receberÃ¡
        messages = [system_prompt, HumanMessage(content=user_message.content)]

        # ðŸ”¹ Enviamos para o LLM e processamos a resposta
        response = model.invoke(messages).content.strip()

        # ðŸ”¹ Se a resposta for um JSON vÃ¡lido, a pergunta estÃ¡ pronta para o prÃ³ximo nÃ³
        if response.startswith("{") and "is_relevant" in response:
            state["messages"].append(AIMessage(content="âœ… Entendi sua pergunta! Vou processÃ¡-la."))
            return state | json.loads(response)

        # ðŸ”¹ Se ainda nÃ£o for uma pergunta vÃ¡lida, continuamos no mesmo nÃ³
        state["messages"].append(AIMessage(content=response))
        return state

    def classify_query(self, state: dict) -> dict:
        """
        Classifica se a pergunta Ã© relevante para e-commerce.
        Se for relevante, avanÃ§a para o refinamento; caso contrÃ¡rio, encerra a interaÃ§Ã£o.
        """
        user_query = state["messages"][-1].content  # Ãšltima mensagem do usuÃ¡rio

        # ClassificaÃ§Ã£o para saber se a pergunta Ã© sobre e-commerce
        classification_prompt = get_classification_prompt(user_query)
        response = model.invoke([HumanMessage(content=classification_prompt)]).content.strip()

        is_valid = response.upper() == "SIM"

        if is_valid:
            # Se a pergunta for vÃ¡lida para e-commerce, seguimos para refinamento
            state["messages"].append(
                AIMessage(content="âœ… Sua pergunta Ã© relevante para e-commerce. Vamos refinÃ¡-la se necessÃ¡rio.")
            )
            return state | {"is_valid_query": True}

        # Se for irrelevante, finalizamos o fluxo
        state["messages"].append(
            AIMessage(
                content="âŒ Desculpe, sÃ³ posso responder perguntas relacionadas a vendas, produtos ou faturamento de e-commerces."
            )
        )

        return state | {"is_valid_query": False}

    def analyze_tables(self, state: dict) -> dict:
        """
        ObtÃ©m informaÃ§Ãµes das tabelas e adiciona ao estado.
        """
        state["messages"].append(AIMessage(content="ðŸ” Obtendo informaÃ§Ãµes das tabelas relevantes..."))

        tools = self.toolkit.get_tools()
        get_schema_tool = next(tool for tool in tools if tool.name == "sql_db_schema")

        tables = ["orders_ia", "orders_items_ia"]
        table_schemas = {}

        for table in tables:
            try:
                full_info = get_schema_tool.invoke(table)
                table_schemas[table] = full_info
            except Exception:
                table_schemas[table] = "Erro ao obter esquema"

        return state | {"table_schemas": table_schemas}

    def generate_sql(self, state: dict) -> dict:
        """
        Gera uma consulta SQL para responder Ã  pergunta do usuÃ¡rio.
        Se houver erro, adiciona uma mensagem no chat pedindo a correÃ§Ã£o.
        """
        user_query = state["user_query"]
        table_schemas = state.get("table_schemas", {})
        query_error = state.get("query_error")  # Verifica se hÃ¡ erro anterior

        if not table_schemas:
            return state | {"sql_query": "Erro: Nenhuma informaÃ§Ã£o de tabela disponÃ­vel."}

        # ðŸ”¹ Formata o contexto do banco de dados
        schema_context = "\n".join([f"Table {table}: {schema}" for table, schema in table_schemas.items()])

        # ðŸ”¹ Se for a primeira tentativa, adicionamos o prompt original
        if not query_error:
            sql_prompt_text = sql_prompt(schema_context, user_query)
            state["messages"].append(HumanMessage(content=sql_prompt_text))
        else:
            # ðŸ”¹ Se houve erro, adicionamos uma nova mensagem pedindo correÃ§Ã£o
            state["messages"].append(
                HumanMessage(content=f"Sua query retornou com esse erro: {query_error}. Por favor, corrija.")
            )

        # ðŸ”¹ Invocamos o LLM passando o histÃ³rico de mensagens
        llm_response = model.invoke(state["messages"])

        sql_query = llm_response.content.strip()

        if not sql_query.lower().startswith("select"):
            return state | {"sql_query": f"Erro: A query gerada nÃ£o Ã© vÃ¡lida.\nQuery: {sql_query}"}

        # ðŸ”¹ Adicionamos a resposta do LLM no histÃ³rico
        state["messages"].append(AIMessage(content=sql_query))

        return state | {
            "sql_query": sql_query,
            "query_error": None,  # ðŸ”¹ Resetamos o erro apÃ³s a correÃ§Ã£o
            "retry_generate_sql": False,  # ðŸ”¹ Resetamos a flag para evitar loops infinitos
        }

    def validate_sql(self, state: dict) -> dict:
        """
        Valida a query SQL antes da execuÃ§Ã£o usando `QuerySQLCheckerTool`.
        """
        state["messages"].append(AIMessage(content="ðŸ”Ž Validando a query SQL..."))

        query = state.get("sql_query", "")
        validation_result = self.query_checker.run(query)  # ðŸ”¹ Valida a SQL

        # ðŸ”¹ Se a validaÃ§Ã£o encontrar erro, retorna para gerar uma nova query
        if "ERROR:" in validation_result or "SQL state" in validation_result:
            state["messages"].append(AIMessage(content=f"âŒ Erro na validaÃ§Ã£o da query: {validation_result}"))
            return state | {"sql_error": validation_result, "retry_generate_sql": True}

        state["messages"].append(AIMessage(content="âœ… Query SQL validada com sucesso!"))
        return state | {"sql_error": None}  # ðŸ”¹ Reseta qualquer erro anterior

    def execute_sql(self, state: dict) -> dict:
        """
        Executa a consulta SQL no banco de dados e retorna os resultados.
        Se houver erro, volta para o LLM como uma nova mensagem de chat pedindo correÃ§Ã£o.
        """
        state["messages"].append(AIMessage(content="â³ Executando a query no banco de dados..."))

        query = state.get("sql_query", "")
        result = self.db.run_no_throw(query)  # Retorna sempre uma string

        # ðŸ”¹ Se a resposta comeÃ§a com "ERROR:", consideramos uma falha na execuÃ§Ã£o
        if result.startswith("ERROR:") or "SQL state:" in result:
            # ðŸ”¹ Ao invÃ©s de sobrescrever o prompt, adicionamos uma nova mensagem no chat
            state["messages"].append(
                HumanMessage(content=f"Sua query retornou com esse erro: {result}. Por favor, corrija-a.")
            )

            return state | {
                "query_error": result,
                "retry_generate_sql": True,  # ðŸ”¹ Apenas ativamos se houver erro
            }

        state["messages"].append(AIMessage(content="âœ… Consulta SQL executada com sucesso."))

        return state | {
            "query_response": result,
            "retry_generate_sql": False,  # ðŸ”¹ Resetamos para evitar loops
        }

    def generate_answer(self, state: dict) -> dict:
        """
        Gera a resposta final para o usuÃ¡rio baseada na consulta SQL e nos dados retornados.
        """
        state["messages"].append(AIMessage(content="âœ… Processando os resultados da consulta..."))

        sql_query = state.get("sql_query", "")
        query_response = state.get("query_response", [])

        if not query_response or "Erro" in query_response[0]:
            return state | {"final_answer": "âŒ Desculpe, nÃ£o foi possÃ­vel obter uma resposta."}

        answer_prompt = format_answer_prompt(state["user_query"], sql_query, query_response)
        final_answer = model.invoke([HumanMessage(content=answer_prompt)]).content.strip()

        state["messages"].append(AIMessage(content=final_answer))

        return state | {"final_answer": final_answer}

    def choose_visualization(self, state: dict) -> dict:
        """Choose an appropriate visualization for the data."""
        user_query = state["user_query"]
        query_response = state["query_response"]
        sql_query = state["sql_query"]

        # ðŸ”¹ Se query_response estiver vazio ou tiver menos de 3 registros, nÃ£o precisa de visualizaÃ§Ã£o
        if not query_response or len(query_response) < 3:
            return {
                "visualization": "none",
                "visualization_reason": "Os dados retornados sÃ£o insuficientes para uma visualizaÃ§Ã£o significativa.",
            }

        # ðŸ”¹ Obtendo o template do prompt
        prompt_template = get_visualization_prompt()

        # ðŸ”¹ Formatando corretamente para passar ao LLM
        formatted_prompt = prompt_template.format(
            user_query=user_query, sql_query=sql_query, query_response=query_response
        )

        response = model.invoke(formatted_prompt)

        # ðŸ”¹ Garantindo que response Ã© um objeto AIMessage e acessando seu conteÃºdo
        response_text = response.content if hasattr(response, "content") else str(response)

        # ðŸ”¹ Processando a resposta corretamente
        lines = response_text.split("\n")

        if len(lines) < 2:
            return {"visualization": "none", "visualization_reason": "O LLM nÃ£o retornou uma resposta vÃ¡lida."}

        visualization = lines[0].split(": ")[1] if ": " in lines[0] else "none"
        reason = lines[1].split(": ")[1] if ": " in lines[1] else "Motivo nÃ£o identificado"

        return {"visualization": visualization, "visualization_reason": reason}
